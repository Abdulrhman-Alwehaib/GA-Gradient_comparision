{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57d08fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db23541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron():\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "        return (1/1 + torch.exp(-x))\n",
    "\n",
    "    def initializaingweights(self,tensorInput):\n",
    "        if self.weights == None:\n",
    "            self.weights = torch.rand(size=(tensorInput.shape[1],),requires_grad=True)\n",
    "        return self.weights\n",
    "\n",
    "    def initializingBias(self):\n",
    "        if self.bias == None:\n",
    "            self.bias =  torch.rand(size=(1,),requires_grad=True)\n",
    "        return self.bias\n",
    "\n",
    "    def MSE(self,outputTensor,ActualTensort):\n",
    "        amount = len(outputTensor)\n",
    "        SquaredIndiviual = torch.square(outputTensor - ActualTensort)\n",
    "        summation = torch.sum(SquaredIndiviual)\n",
    "        return (1/amount) * summation\n",
    "    \n",
    "    def sinLoss(self, output:torch.Tensor, target:torch.Tensor) -> torch.Tensor:\n",
    "        error = output - target\n",
    "        return torch.mean(torch.sin(200 * error) + 0.5 * error**2)\n",
    "\n",
    "\n",
    "    def feedforward(self,tensorInput,weightss=None,biss=None,hurstics:bool=False):\n",
    "        shapes = tensorInput.shape\n",
    "\n",
    "        if hurstics: \n",
    "            weights = weightss\n",
    "            bias = biss\n",
    "        else:\n",
    "            weights = self.initializaingweights(tensorInput)\n",
    "            bias = self.initializingBias()\n",
    "\n",
    "        multiplications = torch.mul(tensorInput,weights)\n",
    "\n",
    "        summation = torch.sum(multiplications,dim=1)\n",
    "\n",
    "        biasAddition = summation.add(bias)\n",
    "\n",
    "        result = biasAddition\n",
    "        \n",
    "        return result\n",
    "    \n",
    "\n",
    "\n",
    "    def optimization(self,loss:torch.Tensor):\n",
    "        LR = 0.001\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            ResultGradientWRTWeight = self.weights.grad\n",
    "            ResultGradientWRTBias = self.bias.grad\n",
    "\n",
    "            self.weights -= LR * (ResultGradientWRTWeight)\n",
    "\n",
    "            self.bias -= LR * (ResultGradientWRTBias)\n",
    "\n",
    "        self.weights.grad.zero_()\n",
    "        self.bias.grad.zero_()\n",
    "    \n",
    "\n",
    "\n",
    "    def fittest(self,input:torch.Tensor,actualOutput:torch.Tensor,weights:torch.Tensor,bias:torch.Tensor):\n",
    "        with torch.no_grad():\n",
    "            Nuronoutput = self.feedforward(input,weights,bias,hurstics=True)\n",
    "            LossError = self.sinLoss(Nuronoutput,actualOutput)\n",
    "        \n",
    "        return LossError.mean().item()\n",
    "    \n",
    "\n",
    "\n",
    "    def GA(self,input:torch.Tensor,actuaOutput:torch.Tensor,Initial_mutation_rate = 0.2,final_mutationRate = 0.05,populationSize = 200,generation = 100):\n",
    "        \n",
    "        population = [\n",
    "            (torch.rand(input.shape[1],) * 2 - 1,torch.rand(1,)* 2 - 1) for _ in range(populationSize)\n",
    "        ]\n",
    "        initial_mutationRate = Initial_mutation_rate\n",
    "        final_mutationRate = final_mutationRate\n",
    "        \n",
    "\n",
    "        for Current_gen in range(generation):\n",
    "            \n",
    "            CurrentMutation_rate = initial_mutationRate - (initial_mutationRate - final_mutationRate) * (Current_gen/generation)\n",
    "            \n",
    "            fittestScores = [\n",
    "                self.fittest(input,actuaOutput,parameters[0],parameters[1]) for parameters in population\n",
    "            ]\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "            sortedPop = [x for _,x in sorted(zip(fittestScores,population),key=lambda x: x[0])]\n",
    "            survivedPop = sortedPop[:math.ceil(populationSize * 0.5)]\n",
    "\n",
    "\n",
    "           \n",
    "            newPop = survivedPop.copy()\n",
    "            while len(newPop) < populationSize:\n",
    "                parentOne,parentTwo = random.sample(survivedPop,2)\n",
    "\n",
    "                childWeight = (parentOne[0] + parentTwo[0]) / 2\n",
    "                childBias = (parentOne[1] + parentTwo[1]) / 2\n",
    "\n",
    "                if random.random() < CurrentMutation_rate:\n",
    "                    childWeight += torch.randn_like(childWeight) * 0.2\n",
    "                if random.random() < CurrentMutation_rate:\n",
    "                    childBias += torch.randn_like(childBias) * 0.2\n",
    "\n",
    "                newPop.append((childWeight,childBias))\n",
    "\n",
    "                \n",
    "\n",
    "            population = newPop\n",
    "\n",
    "            bestIndex = torch.tensor(fittestScores).argmin().item()\n",
    "            bestParameters = population[bestIndex]\n",
    "            bestLoss = fittestScores[bestIndex]\n",
    "            \n",
    "            self.weights = bestParameters[0]\n",
    "            self.bias = bestParameters[1]\n",
    "        return bestLoss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e39cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "SD = StandardScaler()\n",
    "\n",
    "df = pd.read_csv(\"./Housing.csv\")\n",
    "\n",
    "df = df.drop(columns=[\"mainroad\",\"guestroom\",\"basement\",\"hotwaterheating\",\"airconditioning\",\"parking\",\"prefarea\",\"furnishingstatus\"],axis=1)\n",
    "XScaled = SD.fit_transform(df)\n",
    "\n",
    "inputTensor = XScaled[:,:-1]\n",
    "outputTensor = XScaled[:,-1]\n",
    "\n",
    "inputTensor = torch.tensor(inputTensor)\n",
    "outputTensor = torch.tensor(outputTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1cebf002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generatic Algorthim Average Time: 3.5551 Seconds\n",
      "Generatic Algorthim Error: 0.244104\n",
      "------------------------------------------------------------------------------\n",
      "Gradient Average Time: 12.2340 Seconds\n",
      "Gradient Error: 0.334265\n",
      "------------------------------------------------------------------------------\n",
      "DM-GA is faster by 29.06%\n",
      "DM-GA is better convergence by 73.03%\n"
     ]
    }
   ],
   "source": [
    "GA_times = []\n",
    "NumOfTrials = 1\n",
    "\n",
    "for _ in range(NumOfTrials):\n",
    "\n",
    "    model = perceptron()\n",
    "    startTime = time.perf_counter()\n",
    "    lossHurstics = model.GA(inputTensor,outputTensor)\n",
    "\n",
    "    endTime = time.perf_counter()\n",
    "    GA_times.append(endTime - startTime)\n",
    "GA_Average_Time = torch.Tensor(GA_times).mean()\n",
    "\n",
    "print(f\"Generatic Algorthim Average Time: {GA_Average_Time:.4f} Seconds\")\n",
    "print(f\"Generatic Algorthim Error: {lossHurstics:.6f}\")\n",
    "\n",
    "\n",
    "print(\"------\" * 13)\n",
    "\n",
    "\n",
    "Gradient_Times = []\n",
    "for _ in range(NumOfTrials):\n",
    "    NotStuck = True\n",
    "    model = perceptron()\n",
    "    i = 0\n",
    "    startTime = time.perf_counter()\n",
    "    while NotStuck:\n",
    "        i += 1\n",
    "        output = model.feedforward(inputTensor)\n",
    "        PrevsinError = model.sinLoss(output,outputTensor)\n",
    "        model.optimization(PrevsinError)\n",
    "        output = model.feedforward(inputTensor)\n",
    "        sinError = model.sinLoss(output,outputTensor)\n",
    "        if round(float(PrevsinError),5) == round(float(sinError),5):\n",
    "            NotStuck = False\n",
    "        \n",
    "\n",
    "    endTime = time.perf_counter()\n",
    "    Gradient_Times.append(endTime - startTime)\n",
    "\n",
    "GD_Average_Time = torch.Tensor(Gradient_Times).mean()\n",
    "GA_loss = model.sinLoss(output=output,target=outputTensor)\n",
    "print(f\"Gradient Average Time: {GD_Average_Time:.4f} Seconds\")\n",
    "print(f\"Gradient Error: {GA_loss:.6f}\")\n",
    "\n",
    "print(\"------\" * 13)\n",
    "\n",
    "print(f\"DM-GA is faster by {(GA_Average_Time / GD_Average_Time) * 100:.2f}%\" if GA_Average_Time < GD_Average_Time else f\"GD is faster by {(GD_Average_Time / GA_Average_Time) * 100:.2f}%\")\n",
    "print(f\"DM-GA is better convergence by {(lossHurstics / GA_loss) * 100:.2f}%\" if lossHurstics < GA_loss else f\"GD is better convergence by {(GA_loss / lossHurstics) * 100:.2f}%\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
